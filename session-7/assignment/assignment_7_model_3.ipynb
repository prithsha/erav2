{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../data\"\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_MNIST_data_stats(image_dataset : datasets.MNIST):\n",
    "\n",
    "    data_set = image_dataset.data\n",
    "    numpy_data = image_dataset.transform( data_set.numpy())\n",
    "    \n",
    "    print(\"--STATS--\")\n",
    "    print(f\"Numpy shape: {data_set.cpu().numpy().shape}\")\n",
    "    print(f\"Tensor shape: {data_set.size()}\")\n",
    "    print(f\"Min: {torch.min(numpy_data)}\")\n",
    "    print(f\"Max: {torch.max(numpy_data)}\")\n",
    "    print(f\"mean: {torch.mean(numpy_data)}\")\n",
    "    print(f\"std: {torch.std(numpy_data)}\")\n",
    "    print(f\"var: {torch.var(numpy_data)}\")\n",
    "\n",
    "\n",
    "\n",
    "def show_images(image_data, rows=4, cols=4):\n",
    "    print(f\"Number of images: {len(image_data)}\")\n",
    "    sample_idx = random.randint(0,len(image_data))\n",
    "    # print(training_data[0])\n",
    "    print(f\"Image index: {sample_idx}, label : {image_data[sample_idx][1]}, image size: {image_data[sample_idx][0].shape}\")\n",
    "\n",
    "    # Showing one image\n",
    "    plt.imshow(image_data[sample_idx][0].squeeze(),cmap=\"gray_r\")\n",
    "\n",
    "    figure = plt.figure(figsize=(6, 6))\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(image_data), size=(1,)).item()\n",
    "        img, label = image_data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        \n",
    "        plt.title(label)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_transforms(only_basic = True):\n",
    "    if only_basic:\n",
    "        return transforms.Compose([ transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])\n",
    "    else:\n",
    "        \n",
    "        return transforms.Compose(\n",
    "            [ transforms.ToTensor(),\n",
    "             transforms.Normalize((0.1307,),(0.3081,)),\n",
    "             transforms.RandomRotation((-7.0,7.0), fill=(1,))\n",
    "            #  transforms.RandomApply([transforms.RandomRotation(degrees=10)],p=0.1)\n",
    "             #transforms.RandomApply([transforms.RandomCrop(size=(2,2), fill=0)],p=0.1)\n",
    "            #  transforms.RandomErasing(p=0.1, scale=(0.02,0.1))\n",
    "            #  transforms.RandomHorizontalFlip(p=0.1)\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download training data from open datasets.\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=DATA_FOLDER,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=get_custom_transforms(only_basic=True)\n",
    ")\n",
    "\n",
    "\n",
    "# Download test data from open datasets.\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=DATA_FOLDER,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=get_custom_transforms(only_basic=True)\n",
    ")\n",
    "\n",
    "print(\"---->TRAINING data with transform\")\n",
    "get_MNIST_data_stats(train_dataset)\n",
    "show_images(train_dataset)\n",
    "\n",
    "\n",
    "print(\"---->TEST data with basic transform\")\n",
    "get_MNIST_data_stats(test_dataset)\n",
    "show_images(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gpu_available = torch.cuda.is_available()\n",
    "SEED = 1\n",
    "device = \"cpu\"\n",
    "if(is_gpu_available):\n",
    "    device = \"cuda\"\n",
    "    # This ensures that computations involving randomness on the GPU will produce the same results\n",
    "    # when the seed is the same, even if you run the code multiple times.\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "for data, target in test_dataloader:\n",
    "#  X would likely be a tensor containing the pixel values of a batch of grayscale images \n",
    "# (e.g., shape: (batch_size, 1 channel, 28, 28)).\n",
    "# y would contain the corresponding labels for each image, indicating the actual digit represented in the image \n",
    "# (e.g., a tensor of shape (batch_size,) containing integer values between 0 and 9).\n",
    "    print(f\"Shape of X [Batch, C, H, W]: {data.shape}\")\n",
    "    print(f\"Shape of y (label): {target.shape} {target.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, input_size=(1,28,28)):\n",
    "    summary(model=model, input_size=input_size)   \n",
    "    params = model.state_dict()\n",
    "    total_parameters =  sum(p.numel() for p in params.values())\n",
    "    print(f\"Total parameters : {total_parameters}\")\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With regularization . Adding dropout \n",
    "# Adding global average pooling and capacity at bottom layer\n",
    "DROP_OUT = 0.1\n",
    "class Net2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,padding=0, bias=False),            \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#26\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,out_channels=12,kernel_size=3,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#24 RF-5\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=12,out_channels=16,kernel_size=3,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#22, RF-7\n",
    "\n",
    "        # Max pool\n",
    "        self.pool1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )#11, RF-8\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=8,kernel_size=1,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#11, RF-8\n",
    "        \n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,out_channels=8,kernel_size=3,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#9, 12\n",
    "\n",
    "        self.conv_block5_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,out_channels=8,kernel_size=3,padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#9, 12\n",
    "\n",
    "        self.conv_block6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#7\n",
    "\n",
    "        self.conv_block7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=16,kernel_size=3,padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(DROP_OUT)\n",
    "        )#5\n",
    "\n",
    "\n",
    "\n",
    "        # Output block\n",
    "\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=5)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv_block8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=10,kernel_size=1,padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x) \n",
    "        \n",
    "        x = self.pool1(x)\n",
    "              \n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        x = self.conv_block5_1(x)\n",
    "        x = self.conv_block6(x)\n",
    "        x = self.conv_block7(x)\n",
    "        \n",
    "        x = self.gap(x)\n",
    "        x = self.conv_block8(x)\n",
    "        \n",
    "        x = x.view(-1,10)\n",
    "       \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance(index=0) -> nn.Module:\n",
    "    if(index <= 2 ):\n",
    "        return Net2().to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PARAMETER_COUNT = 20000\n",
    "\n",
    "model = get_model_instance(index=2)\n",
    "\n",
    "actual_parameter_count = count_parameters(model)\n",
    "IS_OK = False\n",
    "if(actual_parameter_count <= TARGET_PARAMETER_COUNT):    \n",
    "    IS_OK = True\n",
    "\n",
    "print(f\"Parameters criteria IS_OK: {IS_OK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot accuracy and loss graphs\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}\n",
    "\n",
    "def GetCorrectPredCount(pPrediction, pLabels):\n",
    "  return pPrediction.argmax(dim=1).eq(pLabels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Compute prediction and loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = loss_fn(pred, target)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += GetCorrectPredCount(pred, target)\n",
    "        processed += len(data)\n",
    "        pbar.set_description(desc= f'Train: Loss={loss.item():0.4f} Batch_id={batch_id} Accuracy={100*correct/processed:0.2f}')\n",
    "\n",
    "    train_acc.append(100*correct/processed)\n",
    "    train_losses.append(train_loss/len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            pred = model(data)\n",
    "            test_loss += loss_fn(pred, target).item()\n",
    "            correct += GetCorrectPredCount(pred, target)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc.append(100. * correct / len(dataloader.dataset))\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(dataloader.dataset),\n",
    "        100. * correct / len(dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 15\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1, verbose=True)\n",
    "\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    scheduler.step()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
